# ðŸš¨ Runbook - Alert Response Procedures

This runbook provides step-by-step procedures for responding to alerts generated by the AI DevOps Brain platform.

---

## ðŸ”´ Critical Alerts

### Critical CPU Usage

**Alert**: `CriticalCPUUsage`  
**Severity**: Critical  
**Threshold**: CPU usage > 95% for 2 minutes

#### Symptoms
- Pod CPU usage consistently above 95%
- Application may be unresponsive
- High latency in requests

#### Investigation Steps
1. **Check pod status**:
   ```bash
   kubectl get pods -n <namespace> -o wide
   kubectl top pod -n <namespace>
   ```

2. **Check pod logs**:
   ```bash
   kubectl logs <pod-name> -n <namespace> --tail=100
   ```

3. **Check resource limits**:
   ```bash
   kubectl describe pod <pod-name> -n <namespace> | grep -A 5 "Limits"
   ```

#### Remediation Steps
1. **Immediate**:
   - Scale up deployment: `kubectl scale deployment <deployment> -n <namespace> --replicas=<new-count>`
   - Or increase CPU limits in deployment manifest

2. **Short-term**:
   - Check for runaway processes
   - Review application code for CPU-intensive operations
   - Consider horizontal pod autoscaling (HPA)

3. **Long-term**:
   - Right-size CPU requests/limits based on actual usage
   - Optimize application code
   - Consider vertical pod autoscaling (VPA)

#### Prevention
- Set appropriate CPU limits
- Monitor CPU usage trends
- Implement HPA based on CPU metrics

---

### Critical Memory Usage

**Alert**: `CriticalMemoryUsage`  
**Severity**: Critical  
**Threshold**: Memory usage > 95% of limit for 2 minutes

#### Symptoms
- Pod memory usage near limit
- Risk of OOM (Out of Memory) kill
- Application may crash

#### Investigation Steps
1. **Check memory usage**:
   ```bash
   kubectl top pod -n <namespace>
   kubectl describe pod <pod-name> -n <namespace> | grep -A 5 "Memory"
   ```

2. **Check for memory leaks**:
   ```bash
   kubectl logs <pod-name> -n <namespace> | grep -i "memory\|oom\|out of memory"
   ```

3. **Check container memory stats**:
   ```bash
   kubectl exec <pod-name> -n <namespace> -- cat /sys/fs/cgroup/memory/memory.usage_in_bytes
   ```

#### Remediation Steps
1. **Immediate**:
   - Increase memory limit in deployment
   - Restart pod: `kubectl delete pod <pod-name> -n <namespace>`
   - Scale up deployment to distribute load

2. **Short-term**:
   - Identify memory leak in application
   - Review application memory usage patterns
   - Check for memory-intensive operations

3. **Long-term**:
   - Fix memory leaks in code
   - Right-size memory requests/limits
   - Implement memory-based HPA

#### Prevention
- Set appropriate memory limits with headroom
- Monitor memory usage trends
- Implement memory leak detection
- Regular memory profiling

---

### Pod Crash Looping

**Alert**: `PodCrashLooping`  
**Severity**: Critical  
**Threshold**: Pod restarts > 0 in 15 minutes

#### Symptoms
- Pod continuously restarting
- Application unavailable
- High restart count

#### Investigation Steps
1. **Check pod status**:
   ```bash
   kubectl get pods -n <namespace>
   kubectl describe pod <pod-name> -n <namespace>
   ```

2. **Check pod logs**:
   ```bash
   kubectl logs <pod-name> -n <namespace> --previous
   kubectl logs <pod-name> -n <namespace> --tail=100
   ```

3. **Check events**:
   ```bash
   kubectl get events -n <namespace> --sort-by='.lastTimestamp' | grep <pod-name>
   ```

4. **Check container exit codes**:
   ```bash
   kubectl describe pod <pod-name> -n <namespace> | grep -A 10 "Last State"
   ```

#### Remediation Steps
1. **Immediate**:
   - Check application logs for errors
   - Verify container image is correct
   - Check resource limits (CPU/memory)
   - Verify environment variables and secrets

2. **Common Causes**:
   - **Application error**: Fix code bug
   - **Image pull error**: Verify image exists and is accessible
   - **Resource limits**: Increase CPU/memory limits
   - **Config error**: Fix configuration files
   - **Dependency issue**: Check database/API connectivity

3. **Short-term**:
   - Rollback to previous working version
   - Scale down to prevent resource exhaustion
   - Check for dependency failures

4. **Long-term**:
   - Implement health checks
   - Add better error handling
   - Improve application logging
   - Add startup probes

#### Prevention
- Comprehensive health checks
- Proper error handling
- Resource limits with headroom
- Image versioning and testing
- Dependency health monitoring

---

### Pod Failed

**Alert**: `PodFailed`  
**Severity**: Critical  
**Threshold**: Pod in Failed state for 5 minutes

#### Symptoms
- Pod in Failed state
- Application unavailable
- No new pods starting

#### Investigation Steps
1. **Check pod status**:
   ```bash
   kubectl get pods -n <namespace>
   kubectl describe pod <pod-name> -n <namespace>
   ```

2. **Check events**:
   ```bash
   kubectl get events -n <namespace> --sort-by='.lastTimestamp'
   ```

3. **Check deployment**:
   ```bash
   kubectl describe deployment <deployment> -n <namespace>
   ```

#### Remediation Steps
1. **Immediate**:
   - Delete failed pod to trigger recreation
   - Check deployment configuration
   - Verify image and resources

2. **Common Causes**:
   - **Image pull failure**: Fix image registry/auth
   - **Startup failure**: Fix application startup code
   - **Resource exhaustion**: Increase limits
   - **Node issues**: Check node health

3. **Short-term**:
   - Rollback deployment
   - Fix root cause
   - Scale deployment

#### Prevention
- Image availability checks
- Startup probe configuration
- Resource limit validation
- Node health monitoring

---

## âš ï¸ Warning Alerts

### High CPU Usage

**Alert**: `HighCPUUsage`  
**Severity**: Warning  
**Threshold**: CPU usage > 80% for 5 minutes

#### Investigation
1. Check CPU usage trends
2. Identify CPU-intensive operations
3. Review application performance

#### Remediation
- Consider scaling up
- Optimize CPU-intensive code
- Review resource allocation

---

### High Memory Usage

**Alert**: `HighMemoryUsage`  
**Severity**: Warning  
**Threshold**: Memory usage > 85% of limit for 5 minutes

#### Investigation
1. Check memory usage trends
2. Look for memory leaks
3. Review memory allocation

#### Remediation
- Increase memory limits
- Fix memory leaks
- Optimize memory usage

---

### Excessive Pod Restarts

**Alert**: `ExcessivePodRestarts`  
**Severity**: Warning  
**Threshold**: > 5 restarts in 1 hour

#### Investigation
1. Check restart reasons
2. Review pod logs
3. Check resource limits

#### Remediation
- Fix root cause
- Adjust resource limits
- Improve health checks

---

## ðŸ“Š Alert Monitoring

### Viewing Alerts

1. **Prometheus UI**:
   ```bash
   kubectl port-forward svc/prometheus-kube-prometheus-prometheus -n monitoring 9090:9090
   # Open: http://localhost:9090/alerts
   ```

2. **Alertmanager UI**:
   ```bash
   kubectl port-forward svc/alertmanager-main -n monitoring 9093:9093
   # Open: http://localhost:9093
   ```

3. **Grafana**:
   - Alerts are visible in Grafana dashboards
   - Configure alert notifications in Grafana

### Alert Routing

Alerts are routed based on:
- **Severity**: Critical vs Warning
- **Component**: CPU, Memory, Pod, etc.
- **Namespace**: Industry-specific routing

### Alert Grouping

Alerts are grouped by:
- Alert name
- Namespace
- Severity

This prevents alert fatigue and provides better context.

---

## ðŸ”§ Alert Configuration

### Modifying Alert Thresholds

Edit `k8s/observability/prometheus-alerts.yaml` and apply:
```bash
kubectl apply -f k8s/observability/prometheus-alerts.yaml
```

### Adding New Alerts

1. Add new rule to `prometheus-alerts.yaml`
2. Apply configuration
3. Update this runbook

### Alert Notifications

Configure in `k8s/observability/alertmanager-config.yaml`:
- Slack webhooks
- Email notifications
- PagerDuty integration
- Custom webhooks

---

## ðŸ“š Additional Resources

- [Prometheus Alerting Documentation](https://prometheus.io/docs/alerting/latest/overview/)
- [Alertmanager Configuration](https://prometheus.io/docs/alerting/latest/configuration/)
- [Kubernetes Troubleshooting](https://kubernetes.io/docs/tasks/debug/)

---

**Last Updated**: December 8, 2024

